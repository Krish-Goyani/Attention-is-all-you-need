{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n        \n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport math\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Embeddings","metadata":{}},{"cell_type":"code","source":"class InputEmbeddings(nn.Module):\n    def __init__(self,d_model:int, vocab_size : int):\n        super().__init__()\n        self.d_model = d_model\n        self.vocab_size = vocab_size\n        self.embedding = nn.Embedding(vocab_size, d_model)\n        \n    def forward(self,x):\n        return self.embedding(x) * math.sqrt(self.d_model)\n\n    \n#════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self,d_model:int, seq_len: int, dropout:float):\n        super().__init__()\n        self.d_model= d_model\n        self.seq_len = seq_len\n        self.dropout = nn.Dropout(dropout)\n        \n        \n        # create matrix of size (seq_len,d_model)\n        pe = torch.zeros(seq_len, d_model)\n        \n        # create matrix of size (seq_len,1)\n        position = torch.arange(0,seq_len,dtype=torch.float).unsqueeze(1)\n        # here in division term i have used log to provid better number stability in the origional papper log is not used\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # (d_model / 2)\n        # Apply sine to even indices\n        pe[:, 0::2] = torch.sin(position * div_term) # sin(position * (10000 ** (2i / d_model))\n        # Apply cosine to odd indices\n        pe[:, 1::2] = torch.cos(position * div_term) # cos(position * (10000 ** (2i / d_model))\n        pe = pe.unsqueeze(0)\n        \n        self.register_buffer('pe',pe)\n        \n    def forward(self,x):\n        x = x + (self.pe[:,:x.shape[1],:]).requires_grad_(False)\n        return self.dropout(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Components of EncoderBlock","metadata":{}},{"cell_type":"code","source":"class LayerNormalization(nn.Module):\n    def __init__(self,features: int,eps:float=10**-6):\n        super().__init__()\n     \n        self.eps = eps\n        self.alpha = nn.Parameter(torch.zeros(features)) # mutlipled\n        self.bias = nn.Parameter(torch.zeros(features)) # added\n        \n    def forward(self,x):\n        mean = x.mean(dim = -1,keepdim=True)\n        std = x.std(dim = -1,keepdim=True)        \n        return self.alpha * (x - mean) / (std + self.eps) + self.bias\n\n\n\nclass FeedForwardBlock(nn.Module):\n    def __init__(self, d_model:int, d_ff : int, dropout:float):\n        super().__init__()\n        \n        self.linear_1 = nn.Linear(d_model, d_ff) # w1 b1\n        self.dropout = nn.Dropout(dropout)\n        self.linear_2 = nn.Linear(d_ff, d_model) # w2 b2\n        \n    def forward(self,x):\n        # (batch, seq_len, d_model) --> (batch, seq_len, d_ff) --> (batch, seq_len, d_model)\n        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))\n    \n\nclass MultiHeadAttentionBlock(nn.Module):\n    def __init__(self,d_model:int,h :int,dropout:float):\n        super().__init__()\n        self.d_model = d_model\n        self.h = h\n   \n        # Make sure d_model is divisible by h\n        assert d_model % h == 0, \"d_model is not divisible by h\"\n        \n        self.d_k = d_model // h # Dimension of vector seen by each head\n        self.w_q = nn.Linear(d_model, d_model, bias=False) # Wq\n        self.w_k = nn.Linear(d_model, d_model, bias=False) # Wk\n        self.w_v = nn.Linear(d_model, d_model, bias=False) # Wv\n        self.w_o = nn.Linear(d_model, d_model, bias=False) # Wo\n        self.dropout = nn.Dropout(dropout)\n        \n    @staticmethod\n    def attention(query, key, value, mask, dropout:nn.Dropout):\n        \"\"\"\n        it will return attention score that's used for visiaization \n        also x that work as input to next element\n        \"\"\"\n        d_k = query.shape[-1]\n        # (batch, h, seq_len, d_k) --> (batch, h, seq_len, seq_len)\n        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n        \n        if mask is not None:\n            attention_scores.masked_fill_(mask == 0, -1e9) #(batch, h, seq_len, seq_len)\n            \n        attention_scores = attention_scores.softmax(dim=-1)\n        \n        if dropout is not None:\n            attention_scores = dropout(attention_scores)\n            \n        # (batch, h, seq_len, seq_len) --> (batch, h, seq_len, d_k)   \n        return (attention_scores @ value), attention_scores\n        \n    def forward(self, q, k, v, mask):\n        query = self.w_q(q) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n        key = self.w_k(k) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n        value = self.w_v(v) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n        \n        #(batch, seq_len, d_model) --> (batch, seq_len, h, d_k) --> (batch, h, seq_len, d_k)\n        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1,2)\n        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)\n        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)\n        \n        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n        \n        # Combine all the heads together\n        # (batch, h, seq_len, d_k) --> (batch, seq_len, h, d_k) --> (batch, seq_len, d_model)\n        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n\n        # Multiply by Wo\n        # (batch, seq_len, d_model) --> (batch, seq_len, d_model)  \n        return self.w_o(x)\n    \n\nclass ResidualConnection(nn.Module):\n    def __init__(self,features: int, dropout: float):\n        super().__init__()\n\n        self.dropout = nn.Dropout(dropout)\n        self.norm = LayerNormalization(features)\n        \n    def forward(self,x, sublayer):\n        \n        \"\"\"\n        sublayer : intermediate layer  \n        \"\"\"\n        return x + self.dropout(sublayer(self.norm(x)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Encoder block & Encoder","metadata":{}},{"cell_type":"code","source":"class EncoderBlock(nn.Module):\n    def __init__(self,  features: int,self_attention_block:MultiHeadAttentionBlock, feed_forward_block : FeedForwardBlock, dropout:float):\n        super().__init__()\n        self.self_attention_block = self_attention_block\n        self.feed_forward_block = feed_forward_block\n        self.residual_connections = nn.ModuleList([ResidualConnection(features,dropout) for _ in range(2)])\n        \n    def forward(self, x, src_mask):\n        \"\"\"src_mask is applied to the i/p of encoder block it's used to prevent padded word to interect with other words\"\"\"\n        #below lambda is used bcz we want first attention block and ff block executed firstly.\n        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x,x,x,src_mask))\n        x = self.residual_connections[1](x, self.feed_forward_block)\n        return x\n        \n        \nclass Encoder(nn.Module):\n    def __init__(self,features: int,layers:nn.ModuleList):\n        super().__init__()\n        self.layers = layers\n        self.norm = LayerNormalization(features)\n        \n    def forward(self, x , mask):\n        for layer in self.layers:\n            x  = layer(x,mask)\n            \n        return self.norm(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DecoderBlock & Decoder","metadata":{}},{"cell_type":"code","source":"class DecoderBlock(nn.Module):\n    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, cross_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n        super().__init__()\n        self.self_attention_block = self_attention_block\n        self.cross_attention_block = cross_attention_block\n        self.feed_forward_block = feed_forward_block\n        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(3)])\n        \n    def forward(self,x, encoder_output, src_mask, tgt_mask):\n        \"\"\"src mask is mask applied to encoder and tgt mask is mask applied to deocder\"\"\"\n        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))\n        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n        x = self.residual_connections[2](x, self.feed_forward_block)\n        return x\n\n    \nclass Decoder(nn.Module):\n    def __init__(self,features: int,layers: nn.ModuleList):\n        super().__init__()\n        self.layers = layers\n        self.norm = LayerNormalization(features)\n        \n    def forward(self,x, encoder_output, src_mask, tgt_mask):\n        for layer in self.layers:\n            x = layer(x,encoder_output, src_mask, tgt_mask)\n\n        return self.norm(x)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# parts apart from encoder & decoder","metadata":{}},{"cell_type":"code","source":"class ProjectionLayer(nn.Module):\n    def __init__(self, d_model, vocab_size):\n        super().__init__()\n        self.proj = nn.Linear(d_model, vocab_size)\n        \n    def forward(self,x):\n        # (batch, seq_len, d_model) --> (batch, seq_len, vocab_size)\n        return torch.log_softmax(self.proj(x), dim=-1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Lord Transformer","metadata":{}},{"cell_type":"code","source":"class Transformer(nn.Module):\n    def __init__(self,\n                    encoder: Encoder,\n                    decoder : Decoder,\n                    src_embed: InputEmbeddings,\n                    tgt_embed: InputEmbeddings,\n                    src_pos: PositionalEncoding,\n                    tgt_pos: PositionalEncoding,\n                    projection_layer: ProjectionLayer):\n        \n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.src_embed = src_embed\n        self.tgt_embed = tgt_embed\n        self.src_pos = src_pos\n        self.tgt_pos = tgt_pos\n        self.projection_layer = projection_layer\n        \n    \"\"\"instead of putting all below components into one forward methos we have implemeted it components wise thats give us data for visualization and bestter reusaboloty\"\"\"\n\n    def encode(self, src, src_mask):\n        src = self.src_pos(self.src_embed(src))\n        return self.encoder(src, src_mask)\n\n    def decode(self, encoder_output: torch.Tensor, src_mask: torch.Tensor, tgt: torch.Tensor, tgt_mask: torch.Tensor):\n        # (batch, seq_len, d_model)   \n        tgt = self.tgt_pos(self.tgt_embed(tgt))\n        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n\n    def project(self,x):\n        return self.projection_layer(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# below method connect all the parts and provides initalization params\ndef build_transformer(src_vocab_size: int, tgt_vocab_size: int, src_seq_len: int, tgt_seq_len: int, d_model: int=512, N: int=6, h: int=8, dropout: float=0.1, d_ff: int=2048) -> Transformer:\n   \n    # Create the embedding layers\n    src_embed = InputEmbeddings(d_model, src_vocab_size)\n    tgt_embed = InputEmbeddings(d_model, tgt_vocab_size)\n\n    # Create the positional encoding layers\n    src_pos = PositionalEncoding(d_model, src_seq_len, dropout)\n    tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout)\n    \n    # Create the encoder blocks\n    encoder_blocks = []\n    for _ in range(N):\n        encoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n        encoder_block = EncoderBlock(d_model, encoder_self_attention_block, feed_forward_block, dropout)\n        encoder_blocks.append(encoder_block)\n\n    # Create the decoder blocks\n    decoder_blocks = []\n    for _ in range(N):\n        decoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n        decoder_cross_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n        decoder_block = DecoderBlock(d_model, decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)\n        decoder_blocks.append(decoder_block)\n        \n    # Create the encoder and decoder\n    encoder = Encoder(d_model, nn.ModuleList(encoder_blocks))\n    decoder = Decoder(d_model, nn.ModuleList(decoder_blocks))\n    \n    # Create the projection layer\n    projection_layer = ProjectionLayer(d_model, tgt_vocab_size)\n    \n    # Create the transformer\n    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer)\n    \n    # Initialize the parameters\n    for p in transformer.parameters():\n        if p.dim() > 1:\n            nn.init.xavier_uniform_(p)\n    \n    return transformer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Config.py","metadata":{}},{"cell_type":"code","source":"from pathlib import Path\n\ndef get_config():\n    return {\n        \"batch_size\": 8,\n        \"num_epochs\": 20,\n        \"lr\": 10**-4,\n        \"seq_len\": 350,\n        \"d_model\": 512,\n        \"datasource\": 'opus_books',\n        \"lang_src\": \"en\",\n        \"lang_tgt\": \"it\",\n        \"model_folder\": \"weights\",\n        \"model_basename\": \"tmodel_\",\n        \"preload\": \"latest\",\n        \"tokenizer_file\": \"tokenizer_{0}.json\",\n        \"experiment_name\": \"runs/tmodel\"\n    }\n\ndef get_weights_file_path(config,epoch:str):\n    model_folder = f\"{config['datasource']}_{config['model_folder']}\"\n    model_filename = f\"{config['model_basename']}{config[epoch]}.pt\"\n    return str(Path('.')/model_folder/model_filename)\n\n# Find the latest weights file in the weights folder\ndef latest_weights_file_path(config):\n    model_folder = f\"{config['datasource']}_{config['model_folder']}\"\n    model_filename = f\"{config['model_basename']}*\"\n    weights_files = list(Path(model_folder).glob(model_filename))\n    \n    if len(weights_files)==0:\n        return None\n    weights_files.sort()\n    return str(weights_files[-1])\n\n\n     ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Datatset.py\n","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\n\ndef causal_mask(size):\n    mask = torch.triu(torch.ones((1, size, size)), diagonal=1).type(torch.int)\n    return mask == 0\n\nclass BilingualDataset(Dataset):\n    def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len):\n        super().__init__()\n        \n        self.seq_len = seq_len\n        self.ds = ds\n        self.tokenizer_src = tokenizer_src\n        self.tokenizer_tgt = tokenizer_tgt\n        self.src_lang = src_lang\n        self.tgt_lang = tgt_lang\n        \n        self.sos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[SOS]\")], dtype=torch.int64)\n        self.eos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[EOS]\")], dtype=torch.int64)        \n        self.pad_token = torch.tensor([tokenizer_tgt.token_to_id(\"[PAD]\")], dtype=torch.int64)        \n        \n    def __len__(self):\n        return len(self.ds)\n    \n    def __getitem__(self, idx):\n        src_target_pair = self.ds[idx]\n        src_text = src_target_pair['translation'][self.src_lang]\n        tgt_text = src_target_pair['translation'][self.tgt_lang]        \n        \n        # Transform the text into tokens\n        enc_input_tokens = self.tokenizer_src.encode(src_text).ids\n        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n        \n        # Add sos, eos and padding to each sentence\n        enc_num_padding_tokens = self.seq_len - len(enc_input_tokens)-2\n        dec_num_padding_tokens = self.seq_len - len(dec_input_tokens)-1\n        \n        # Make sure the number of padding tokens is not negative. If it is, the sentence is too long\n        if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n            raise ValueError(\"Sentence is too long\")\n\n        # Add <s> and </s> token\n        encoder_input = torch.cat(\n            [\n                self.sos_token,\n                torch.tensor(enc_input_tokens, dtype=torch.int64),\n                self.eos_token,\n                torch.tensor([self.pad_token] * enc_num_padding_tokens, dtype=torch.int64),\n            ],\n            dim=0,\n        )\n        \n        # Add only <s> token\n        decoder_input = torch.cat(\n            [\n                self.sos_token,\n                torch.tensor(dec_input_tokens, dtype=torch.int64),\n                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),\n            ],\n            dim=0,\n        )\n        \n        # Add only </s> token\n        label = torch.cat(\n            [\n                torch.tensor(dec_input_tokens, dtype=torch.int64),\n                self.eos_token,\n                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),\n            ],\n            dim=0,\n        )\n        \n        # Double check the size of the tensors to make sure they are all seq_len long\n        assert encoder_input.size(0) == self.seq_len\n        assert decoder_input.size(0) == self.seq_len\n        assert label.size(0) == self.seq_len\n    \n        # what is encoder mask?? we have applied padding in the sentences now task of mask is to ignore the words that are padded in te attention mechanisum for encoder mask all the words which are padded for decoder mask all the words that are padded and words after the current tokens also need to get mask so in deoder current token have access to only non padded tokens before current token\n        \n        return {\n            \"encoder_input\": encoder_input,  # (seq_len)\n            \"decoder_input\": decoder_input,  # (seq_len)\n            \"encoder_mask\": (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(), # (1, 1, seq_len)\n            \"decoder_mask\": (decoder_input != self.pad_token).unsqueeze(0).int() & causal_mask(decoder_input.size(0)), # (1, seq_len) & (1, seq_len, seq_len),\n            \"label\": label,  # (seq_len)\n            \"src_text\": src_text,\n            \"tgt_text\": tgt_text,\n        }\n    \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# train.py","metadata":{}},{"cell_type":"code","source":"import torchtext.datasets as datasets\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torch.optim.lr_scheduler import LambdaLR\n\nimport warnings\nfrom tqdm import tqdm\nimport os\nfrom pathlib import Path\n\n# Huggingface datasets and tokenizers\nfrom datasets import load_dataset\nfrom tokenizers import Tokenizer\nfrom tokenizers.models import WordLevel\nfrom tokenizers.trainers import WordLevelTrainer\nfrom tokenizers.pre_tokenizers import Whitespace\n\nimport torchmetrics\nfrom torch.utils.tensorboard import SummaryWriter","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ndef get_all_sentences(ds, lang):\n    for item in ds:\n        yield item['translation'][lang]\n        \n#════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════\n#════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════\n\n\n\ndef get_or_build_tokenizer(config, ds, lang):\n    tokenizer_path = Path(config['tokenizer_file'].format(lang))\n    \n    if not Path.exists(tokenizer_path):\n        tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n        tokenizer.pre_tokenizer = Whitespace()\n        trainer = WordLevelTrainer(special_tokens = [\"[UNK]\",\"[PAD]\",\"[SOS]\",\"[EOS]\"], min_frequency = 2)\n        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer)\n        tokenizer.save(str(tokenizer_path))\n    else:\n        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n    return tokenizer\n\n#════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════\n#════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════\n\n\n\ndef get_ds(config):\n    # It only has the train split, so we divide it overselves\n    ds_raw = load_dataset(f\"{config['datasource']}\", f\"{config['lang_src']}-{config['lang_tgt']}\", split='train')\n\n    # Build tokenizers\n    tokenizer_src = get_or_build_tokenizer(config, ds_raw, config['lang_src'])\n    tokenizer_tgt = get_or_build_tokenizer(config, ds_raw, config['lang_tgt'])\n\n    # Keep 90% for training, 10% for validation\n    train_ds_size = int(0.9 * len(ds_raw))\n    val_ds_size = len(ds_raw) - train_ds_size\n    train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size])\n\n    train_ds = BilingualDataset(train_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n    val_ds = BilingualDataset(val_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n    \n    # find max length of each sentence in target and source\n    max_len_src=0\n    max_len_tgt=0\n    \n    for item in ds_raw:\n        src_ids = tokenizer_src.encode(item['translation'][config['lang_src']]).ids\n        tgt_ids = tokenizer_tgt.encode(item['translation'][config['lang_tgt']]).ids\n        \n        max_len_src = max(max_len_src, len(src_ids))\n        max_len_tgt = max(max_len_tgt, len(tgt_ids))\n        \n    print(f'Max length of source sentence: {max_len_src}')\n    print(f'Max length of target sentence: {max_len_tgt}')\n    \n    train_dataloader = DataLoader(train_ds, batch_size = config['batch_size'], shuffle =True)\n    val_dataloader = DataLoader(val_ds, batch_size=1, shuffle =True)\n        \n    return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt\n\n#════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════\n#════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════\n\n\n\ndef get_model(config, vocab_src_len, vocab_tgt_len):\n    model = build_transformer(vocab_src_len, \n                              vocab_tgt_len,\n                              config[\"seq_len\"],\n                              config['seq_len'], \n                              d_model=config['d_model'])\n    return model\n\n#════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════\n#════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════\n\n\n\ndef greedy_decode(model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device):\n    sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n    eos_idx = tokenizer_tgt.token_to__id('[EOS]')\n    \n    # Precompute the encoder output and reuse it for every step\n    enocder_output = model.encode(source, source_mask)\n    # initialize the deocder i/p with sos token\n    decoder_input = torch.empty(1,1).fill_(sos_idx).type_as(source).to(device)\n    \n    while True:\n        if decoder_input.size(1) == max_len:\n            break\n            \n        #build mask for target\n        decoder_mask = casual_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n        \n        # output calculation\n        out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n        \n        #get next token\n        prob = model.project(out[:,-1])\n        _, next_word = torch.max(prob, dim=1)\n        decoder_input = torch.cat(\n            [decoder_input, torch.empty(1,1).type_as(source).fill_(next_word.item()).to(device)]\n        )\n        \n        if next_word == eos_idx:\n            break\n            \n    return decoder_input.squeeze(0)\n    \n    \n    \n#════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════\n#════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════\n\n\n    \ndef run_validation(model,\n                  validation_ds,\n                  tokenizer_src,\n                  tokenizer_tgt,\n                  max_len, \n                   device, \n                   print_msg, \n                   global_step,\n                   writer, \n                   num_examples=2):\n    \n    model.eval()\n    count =0\n    \n    source_texts = []\n    expected = []\n    predicted = []\n    try:\n    # get the console window width\n        with os.popen('stty size', 'r') as console:\n            _, console_width = console.read().split()\n            console_width = int(console_width)\n    except:\n        # If we can't get the console width, use 80 as default\n        console_width = 80\n        \n    with torch.no_grad():\n        for batch in validation_ds:\n            count+=1\n            encoder_input = batch[\"encoder_input\"].to(device) # (b, seq_len)\n            encoder_mask = batch[\"encoder_mask\"].to(device) # (b, 1, 1, seq_len)\n\n            assert encoder_input.size(0) == 1 , \"Batch size must be 1 for validation\"\n            \n            model_out = greedy_decode(model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n\n        \n            source_text = batch['src_text'][0]\n            target_text = batch['tgt_text'][0]\n            model_out_text = tokenizer_tgt.decode(model_out.detach().cpu().numpy())\n            \n            source_texts.append(source_text)\n            expected.append(target_text)\n            predicted.append(model_out_text)\n            \n            # Print the source, target and model output\n            print_msg('-'*console_width)\n            print_msg(f\"{f'SOURCE: ':>12}{source_text}\")\n            print_msg(f\"{f'TARGET: ':>12}{target_text}\")\n            print_msg(f\"{f'PREDICTED: ':>12}{model_out_text}\")\n\n            if count == num_examples:\n                print_msg('-'*console_width)\n                break\n    \n    \n#════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════\n#════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════\n\n\n        \ndef train_model(config):\n    #define the device\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    print(\"Using device:\", device)\n    \n    if (device == 'cuda'):\n        print(f\"Device name: {torch.cuda.get_device_name(device.index)}\")\n        print(f\"Device memory: {torch.cuda.get_device_properties(device.index).total_memory / 1024 ** 3} GB\")\n    device  = torch.device(device)\n    \n    #make sure the weight folder exists\n    Path(f\"{config['datasource']}_{config['model_folder']}\").mkdir(parents=True,exist_ok=True)\n    \n    train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n    model = get_model(config,\n                     tokenizer_src.get_vocab_size(),\n                     tokenizer_tgt.get_vocab_size()).to(device)\n    \n    # Tensorboard\n    writer = SummaryWriter(config['experiment_name'])\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], eps=1e-9)\n    \n    # If the user specified a model to preload before training, load it\n    initial_epoch = 0\n    global_step = 0\n    preload = config['preload']\n    model_filename = latest_weights_file_path(config) if preload=='latest' else get_weights_file_path(config, preload) if preload else None\n    if model_filename:\n        pritn(f\"preloading model {model_filename}\")\n        state = torch.load(model_filename)\n        model.load_state_dict(state['model_state_dict'])\n        initial_epoch = state['epoch']+1\n        optimizer.load_state_dict(state['optimizer_state_dict'])\n        global_step = state['global_step']\n    else:\n        print('No model to preload, starting from scratch')\n        \n    loss_fn = nn.CrossEntropyLoss(ignore_index = tokenizer_src.token_to_id('[PAD]'), label_smoothing=0.1).to(device)\n    \n    for epoch in range(initial_epoch, config['num_epochs']):\n        torch.cuda.empty_cache()\n        model.train()\n        \n        batch_iterator = tqdm(train_dataloader, desc=f\"Processing Epoch {epoch:02d}\")\n        for batch in batch_iterator:\n            \n            \n            \n            encoder_input = batch['encoder_input'].to(device) # (b, seq_len)\n            decoder_input = batch['decoder_input'].to(device) # (B, seq_len)\n            encoder_mask = batch['encoder_mask'].to(device) # (B, 1, 1, seq_len)\n            decoder_mask = batch['decoder_mask'].to(device) # (B, 1, seq_len, seq_len)\n            \n            # Run the tensors through the encoder, decoder and the projection layer\n            encoder_output = model.encode(encoder_input, encoder_mask) # (B, seq_len, d_model)\n            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask) # (B, seq_len, d_model)\n            proj_output = model.project(decoder_output) # (B, seq_len, vocab_size)\n            \n            label = batch['label'].to(device) #(b,seq_len)\n            \n            loss = loss_fn(proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))\n            batch_iterator.set_postfix({\"loss\": f\"{loss.item():6.3f}\"})\n            \n            optimizer.zero_grad(set_to_none=True)\n\n            # Log the loss\n            writer.add_scalar('train loss', loss.item(), global_step)\n            writer.flush()\n            \n            loss.backward()\n            \n            optimizer.step()\n            \n            global_step +=1\n            \n        # Run validation at the end of every epoch\n        run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config['seq_len'], device, lambda msg: batch_iterator.write(msg), global_step, writer)\n\n        # Save the model at the end of every epoch\n        model_filename = get_weights_file_path(config, f\"{epoch:02d}\")\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'global_step': global_step\n        }, model_filename)\n    \n    \n#════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════\n#════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════\n\n\nif __name__ == '__main__':\n    warnings.filterwarnings(\"ignore\")\n    config = get_config()\n    train_model(config)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}